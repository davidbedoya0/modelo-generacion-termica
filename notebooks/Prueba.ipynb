{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9186b43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydataxm in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (0.3.15)\n",
      "Requirement already satisfied: plotly in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (6.1.2)\n",
      "Requirement already satisfied: pyarrow in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (20.0.0)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pydataxm) (2.3.0)\n",
      "Requirement already satisfied: numpy in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pydataxm) (2.2.6)\n",
      "Requirement already satisfied: requests in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pydataxm) (2.32.4)\n",
      "Requirement already satisfied: aiohttp in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pydataxm) (3.12.13)\n",
      "Requirement already satisfied: asyncio in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pydataxm) (3.4.3)\n",
      "Requirement already satisfied: nest_asyncio in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pydataxm) (1.6.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from plotly) (1.44.0)\n",
      "Requirement already satisfied: packaging in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pandas>=2.2.3->pydataxm) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pandas>=2.2.3->pydataxm) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from pandas>=2.2.3->pydataxm) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->pydataxm) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from aiohttp->pydataxm) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->pydataxm) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->pydataxm) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from requests->pydataxm) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from requests->pydataxm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/Thermal_Gen_Proy/modelo-generacion-termica/venv/lib/python3.10/site-packages (from requests->pydataxm) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install pydataxm plotly pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62e32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from pydataxm import *\n",
    "from pydataxm.pydatasimem import ReadSIMEM, CatalogSIMEM\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from prophet import Prophet\n",
    "from prophet.utilities import regressor_coefficients\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from json import load\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7562e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalGenerationPipeline:\n",
    "    \"\"\"\n",
    "    A pipeline for fetching, processing, modeling, and forecasting thermal generation data.\n",
    "\n",
    "    The pipeline consists of several stages:\n",
    "    1. Data Acquisition - Fetch data from APIs and external sources\n",
    "    2. Data Processing - Clean and transform the raw data\n",
    "    3. Feature Engineering - Create additional features\n",
    "    4. Modeling - Train and evaluate forecasting models\n",
    "    5. Forecasting - Generate out-of-sample predictions\n",
    "    6. Visualization - Create plots and visualizations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the pipeline with default settings\"\"\"\n",
    "        self.data = None\n",
    "        self.processed_data = None\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.y_mean = None\n",
    "        self.y_std = None\n",
    "        self.results = None\n",
    "        self.model = self.load_serialized_model('serialized_prophet')\n",
    "\n",
    "\n",
    "    def fetch_data(self, list_date, metric_Id_list, metric_Id_list_tar, entity_cov, entity_tar):\n",
    "        \"\"\"\n",
    "        Fetch data from various sources including APIs and external datasets.\n",
    "\n",
    "        Args:\n",
    "            list_date: List containing start and end dates [start_date, end_date]\n",
    "            metric_Id_list: List of metric IDs for covariates\n",
    "            metric_Id_list_tar: Metric ID for target variable\n",
    "            entity_cov: Entity for covariates\n",
    "            entity_tar: Entity for target variable\n",
    "\n",
    "        Returns:\n",
    "            List of raw DataFrames\n",
    "        \"\"\"\n",
    "        df_thermalgen = []\n",
    "\n",
    "        # Fetch covariates data from API\n",
    "        objetoAPI = pydataxm.ReadDB()\n",
    "        for metric_id in metric_Id_list:\n",
    "            df = objetoAPI.request_data(\n",
    "                metric_id,\n",
    "                entity_cov,\n",
    "                dt.date(list_date[0]),\n",
    "                dt.date(list_date[1]))\n",
    "            df_thermalgen.append(df)\n",
    "\n",
    "        # Fetch thermal generation data\n",
    "        dataset_gen = 'E17D25'  # Real Generation from API\n",
    "        simem_gen = ReadSIMEM(dataset_gen, list_date[0], list_date[1])\n",
    "        df_tar = simem_gen.main()\n",
    "        df_thermalgen.append(df_tar)\n",
    "\n",
    "        # Fetch SST data for ONI index\n",
    "        url = \"https://psl.noaa.gov/data/correlation/nina3.anom.data\"\n",
    "        df_sst = self._process_sst_data(url, list_date)\n",
    "        df_thermalgen.append(df_sst)\n",
    "\n",
    "        self.data = df_thermalgen\n",
    "        return df_thermalgen\n",
    "\n",
    "    def _process_sst_data(self, url, date_range):\n",
    "        \"\"\"Process SST data from NOAA\"\"\"\n",
    "        df_sst = pd.read_csv(url, delim_whitespace=True, skiprows=1, header=None)\n",
    "        df_sst_clean = df_sst.iloc[2:-3,:]\n",
    "        df_sst_clean.columns = ['Year', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "\n",
    "        # Reshape and clean data\n",
    "        df_sst_clean_stack = df_sst_clean.set_index(['Year'])[df_sst_clean.columns[1:]].stack().reset_index()\n",
    "        df_sst_clean_stack.columns = ['Year', 'Month', 'ONI Anomaly']\n",
    "        df_sst_clean_stack['Date'] = pd.to_datetime(df_sst_clean_stack[['Year', 'Month']].assign(DAY=1))\n",
    "        df_sst_clean_stack.drop(columns=['Year', 'Month'], inplace=True)\n",
    "        df_sst_clean_stack.set_index('Date', inplace=True)\n",
    "        df_sst_clean_stack['ONI Anomaly'] = pd.to_numeric(df_sst_clean_stack['ONI Anomaly'], errors='coerce')\n",
    "\n",
    "        # Filter by date range and resample\n",
    "        df_sst_clean_stack = df_sst_clean_stack[\n",
    "            (df_sst_clean_stack.index >= date_range[0]) &\n",
    "            (df_sst_clean_stack.index <= date_range[1])\n",
    "        ]\n",
    "        df_sst_clean_stack_daily = df_sst_clean_stack.resample('D').ffill()\n",
    "\n",
    "        # Extend with 30 days of forward fill\n",
    "        new_dates = pd.date_range(\n",
    "            start=df_sst_clean_stack_daily.index[-1] + pd.Timedelta(days=1),\n",
    "            periods=30,\n",
    "            freq='D'\n",
    "        )\n",
    "        df_sst_clean_stack_daily = df_sst_clean_stack_daily.reindex(\n",
    "            df_sst_clean_stack_daily.index.union(new_dates)\n",
    "        ).ffill()\n",
    "\n",
    "        return df_sst_clean_stack_daily\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Process and transform the raw data into a format suitable for modeling.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (daily_data, monthly_data) DataFrames\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data available. Run fetch_data() first.\")\n",
    "\n",
    "        # Unpack the raw data\n",
    "        df_volumen_ener = self.data[0]\n",
    "        df_apor_ener = self.data[1]\n",
    "        df_porc_vol_util_ = self.data[2]\n",
    "        df_genreal = self.data[3]\n",
    "        df_ONI = self.data[4]\n",
    "\n",
    "        # Process generation data\n",
    "        df_genreal_tipo = df_genreal.groupby(['Fecha','TipoGeneracion'])['GeneracionRealEstimada'].sum().reset_index()\n",
    "        Gigafactor_gene = 1e6  # Conversion factor from kWh to Gwh\n",
    "        df_genreal_tipo['GeneracionRealEstimada'] = df_genreal_tipo['GeneracionRealEstimada'].div(Gigafactor_gene)\n",
    "\n",
    "        # Pivot and rename columns\n",
    "        df_genreal_tipo = df_genreal_tipo.pivot_table(\n",
    "            columns='TipoGeneracion',\n",
    "            index='Fecha',\n",
    "            values='GeneracionRealEstimada',\n",
    "            fill_value=0\n",
    "        ).reset_index()\n",
    "        df_genreal_tipo['Fecha'] = pd.to_datetime(df_genreal_tipo['Fecha'], format='%Y-%m-%d')\n",
    "        df_genreal_tipo.set_index('Fecha', inplace=True)\n",
    "        df_genreal_tipo = df_genreal_tipo[['Termica']]\n",
    "        df_genreal_tipo.rename(columns={'Termica':'TERMICA'}, inplace=True)\n",
    "\n",
    "        # Process volume data\n",
    "        df_volumen_ener.rename(columns={'Value':'Volume in Energy'}, inplace=True)\n",
    "        Gigafactor_volu = 1e6\n",
    "        df_volumen_ener['Volume in Energy'] = df_volumen_ener['Volume in Energy'].div(Gigafactor_volu)\n",
    "        df_volumen_ener.drop(columns=['Id'], inplace=True)\n",
    "        df_volumen_ener.set_index('Date', inplace=True)\n",
    "\n",
    "        # Process apor energy data\n",
    "        df_apor_ener.rename(columns={'Value':'AporEnergia'}, inplace=True)\n",
    "        Gigafactor_apor = 1e6\n",
    "        df_apor_ener['AporEnergia'] = df_apor_ener['AporEnergia'].div(Gigafactor_apor)\n",
    "        df_apor_ener.drop(columns=['Id'], inplace=True)\n",
    "        df_apor_ener.set_index('Date', inplace=True)\n",
    "\n",
    "        # Process volume utilization data\n",
    "        df_porc_vol_util_.rename(columns={'Value':'PorcVoluUtilDiario'}, inplace=True)\n",
    "        df_porc_vol_util_.drop(columns=['Id'], inplace=True)\n",
    "        df_porc_vol_util_.set_index('Date', inplace=True)\n",
    "\n",
    "        # Combine all data\n",
    "        df_thermalgen_cov = pd.concat([\n",
    "            df_genreal_tipo,\n",
    "            df_volumen_ener,\n",
    "            df_ONI,\n",
    "            df_apor_ener,\n",
    "            df_porc_vol_util_\n",
    "        ], axis=1)\n",
    "\n",
    "        # Create monthly aggregated data\n",
    "        df_thermalcovariates_monthly = df_thermalgen_cov.resample('M').agg({\n",
    "            'TERMICA': 'sum',\n",
    "            'Volume in Energy': 'sum',\n",
    "            'ONI Anomaly': 'max',\n",
    "            'AporEnergia': 'sum',\n",
    "            'PorcVoluUtilDiario': 'mean'\n",
    "        })\n",
    "\n",
    "        # Reset indices and rename columns\n",
    "        df_thermalgen_cov.reset_index(inplace=True)\n",
    "        df_thermalgen_cov.rename(columns={'index':'Fecha'}, inplace=True)\n",
    "        df_thermalcovariates_monthly.reset_index(inplace=True)\n",
    "        df_thermalcovariates_monthly.rename(columns={'index':'Fecha'}, inplace=True)\n",
    "\n",
    "        self.processed_data = (df_thermalgen_cov, df_thermalcovariates_monthly)\n",
    "        return self.processed_data\n",
    "\n",
    "    def preprocess_for_modeling(self, df, target_col='TERMICA'):\n",
    "        \"\"\"\n",
    "        Prepare data for modeling by scaling and renaming columns.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            target_col: Name of the target column\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (processed DataFrame, scaler object)\n",
    "        \"\"\"\n",
    "        # Store original scale info for inverse transform later\n",
    "        self.y_mean = df[target_col].mean()\n",
    "        self.y_std = df[target_col].std()\n",
    "\n",
    "        # Rename columns for Prophet\n",
    "        df = df.rename(columns={\n",
    "            'Fecha': 'ds',\n",
    "            target_col: 'y',\n",
    "            'Volume in Energy': 'exog2',\n",
    "            'ONI Anomaly': 'exog3',\n",
    "            'AporEnergia': 'exog4',\n",
    "            'PorcVoluUtilDiario': 'exog6',\n",
    "        })\n",
    "\n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        numeric_cols = ['y'] + [f'exog{i}' for i in range(1, 8) if f'exog{i}' in df.columns]\n",
    "        df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "        return df, self.scaler, self.y_mean, self.y_std\n",
    "\n",
    "    def train_model(self, train_df, seasonality_mode='additive',\n",
    "                   weekly_seasonality=False, yearly_seasonality=True):\n",
    "        \"\"\"\n",
    "        Train a Prophet model with optional regressors.\n",
    "\n",
    "        Args:\n",
    "            train_df: Training DataFrame\n",
    "            seasonality_mode: 'additive' or 'multiplicative'\n",
    "            weekly_seasonality: Whether to include weekly seasonality\n",
    "            yearly_seasonality: Whether to include yearly seasonality\n",
    "\n",
    "        Returns:\n",
    "            Trained Prophet model\n",
    "        \"\"\"\n",
    "        regressors = [f'exog{i}' for i in range(1, 8) if f'exog{i}' in train_df.columns]\n",
    "\n",
    "        self.model = Prophet(\n",
    "            seasonality_mode=seasonality_mode,\n",
    "            weekly_seasonality=weekly_seasonality,\n",
    "            yearly_seasonality=yearly_seasonality\n",
    "        )\n",
    "\n",
    "        # Add regressors\n",
    "        for reg in regressors:\n",
    "            self.model.add_regressor(reg)\n",
    "\n",
    "        self.model.fit(train_df)\n",
    "        return self.model\n",
    "\n",
    "    def evaluate_model(self, model, test_df):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data.\n",
    "\n",
    "        Args:\n",
    "            model: Trained Prophet model\n",
    "            test_df: Test DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (forecast DataFrame, metrics dictionary)\n",
    "        \"\"\"\n",
    "        regressors = [f'exog{i}' for i in range(1, 8) if f'exog{i}' in test_df.columns]\n",
    "\n",
    "        # Prepare future dataframe with regressors\n",
    "        future = test_df[['ds'] + regressors].copy()\n",
    "\n",
    "        # Make predictions\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        # Get actual and predicted values\n",
    "        y_true = test_df['y'].values\n",
    "        y_pred = forecast['yhat'].values\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'MSE': mean_squared_error(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        }\n",
    "\n",
    "        return forecast, metrics\n",
    "\n",
    "    def add_lagged_regressors(self, df, regressor_col, lags):\n",
    "        \"\"\"\n",
    "        Add lagged regressors to the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            regressor_col: Name of the regressor column\n",
    "            lags: List of lag values\n",
    "\n",
    "        Returns:\n",
    "            Modified DataFrame\n",
    "        \"\"\"\n",
    "        for exog in regressor_col:\n",
    "          for lag in lags:\n",
    "            df[f'{exog}_lag_t-{lag}'] = df[exog].shift(lag)\n",
    "        return df\n",
    "\n",
    "    def mod_df(self, df, delay, number_of_lags, final_lag):\n",
    "        delay = 1\n",
    "        number_of_lags = 3\n",
    "        final_lag = delay + number_of_lags\n",
    "        lags = np.arange(delay,final_lag)\n",
    "        new_column = 'y_exog'\n",
    "        df[new_column] = df['y']\n",
    "        exog_lagged = ['exog2', 'exog3', 'exog4', 'exog6', 'y_exog']\n",
    "        df = self.add_lagged_regressors(df, exog_lagged, lags)\n",
    "        df.dropna(inplace=True)\n",
    "        df_noexog = df.drop(columns= exog_lagged)\n",
    "        return df_noexog\n",
    "\n",
    "\n",
    "    def rolling_forecast(self, df, horizon=3, train_size=0.7):\n",
    "        \"\"\"\n",
    "        Perform rolling window forecasting.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            horizon: Number of periods to forecast ahead\n",
    "            train_size: Proportion of data to use for initial training\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing forecast results\n",
    "        \"\"\"\n",
    "        # Split data into train and test sets\n",
    "        train_size = int(train_size * len(df))\n",
    "        train = df.iloc[:train_size]\n",
    "        test = df.iloc[train_size:]\n",
    "\n",
    "        # Initialize storage for results\n",
    "        all_predictions = []\n",
    "        all_true_values = []\n",
    "        forecast_dates = []\n",
    "        horizon_labels = []\n",
    "\n",
    "        # Iterate through the test set in steps of 'horizon'\n",
    "        for i in range(0, len(test), horizon):\n",
    "            # Current training data (all past + up to current test point)\n",
    "            current_train = pd.concat([train, test.iloc[:i]], axis=0)\n",
    "\n",
    "            # Test data for the next 'horizon' steps\n",
    "            current_test = test.iloc[i:i + horizon]\n",
    "            if current_test.empty:\n",
    "                break\n",
    "\n",
    "            # Create Prophet Model\n",
    "            model = Prophet(\n",
    "                  yearly_seasonality=True,\n",
    "                  weekly_seasonality=False,\n",
    "                  daily_seasonality=False,\n",
    "                  )\n",
    "            #model = self.train_model(current_train)\n",
    "\n",
    "             # Add all exogenous regressors\n",
    "            regressor_cols = [col for col in current_train.columns if col not in [\"ds\", \"y\"]]\n",
    "            for col in regressor_cols:\n",
    "              model.add_regressor(col)\n",
    "            # Fit model\n",
    "            model.fit(current_train)\n",
    "\n",
    "            future = model.make_future_dataframe(\n",
    "                periods=len(current_test),\n",
    "                freq='M',\n",
    "                include_history=False\n",
    "            )\n",
    "\n",
    "            # Add exogenous variables\n",
    "            future_regressors = current_test[regressor_cols + ['ds']].copy()\n",
    "            future = future.merge(future_regressors, on=\"ds\", how=\"left\")\n",
    "\n",
    "            # Predict\n",
    "            forecast = model.predict(future)\n",
    "\n",
    "            # Store results\n",
    "            all_predictions.extend(forecast[\"yhat\"].values)\n",
    "            all_true_values.extend(current_test[\"y\"].values)\n",
    "            forecast_dates.extend(current_test[\"ds\"].values)\n",
    "            horizon_labels.extend([f\"Month {h + 1}\" for h in range(len(current_test))])\n",
    "\n",
    "            # Serializing the last model trained\n",
    "            #if save_model:\n",
    "            #  with open(save_model, 'wb') as f:\n",
    "            #      pickle.dump(model, f)\n",
    "            #  print(f\"Figure saved to {save_model}\")\n",
    "\n",
    "        self.results = {\n",
    "            \"predictions\": all_predictions,\n",
    "            \"true_values\": all_true_values,\n",
    "            \"forecast_dates\": forecast_dates,\n",
    "            \"horizon_labels\": horizon_labels,\n",
    "            \"model\": model,  # Returns the last trained model\n",
    "            'test_data': test\n",
    "        }\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def create_forecast_results_df(self):\n",
    "        \"\"\"\n",
    "        Create a DataFrame from rolling forecast results.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with forecast results\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No results available. Run rolling_forecast() first.\")\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"ds\": self.results[\"forecast_dates\"],\n",
    "            \"true\": self.results[\"true_values\"],\n",
    "            \"predicted\": self.results[\"predictions\"],\n",
    "            \"horizon\": self.results[\"horizon_labels\"]\n",
    "        })\n",
    "\n",
    "    def calculate_metrics(self, model, results):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for forecast results.\n",
    "\n",
    "        Args:\n",
    "            results: DataFrame with forecast results\n",
    "            model: Optional model for calculating information criteria\n",
    "            end_date_mape: Optional end date for MAPE calculation\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        if results.empty:\n",
    "            print(\"No predictions were made.\")\n",
    "            return None\n",
    "\n",
    "        # Rescale values to original scale\n",
    "        true_rescaled = results[\"true\"] * self.y_std + self.y_mean\n",
    "        pred_rescaled = results[\"predicted\"] * self.y_std + self.y_mean\n",
    "\n",
    "        # Calculate basic metrics\n",
    "        metrics = {\n",
    "            \"rmse\": np.sqrt(mean_squared_error(true_rescaled, pred_rescaled)),\n",
    "            \"mae\": mean_absolute_error(true_rescaled, pred_rescaled)\n",
    "        }\n",
    "\n",
    "        # Calculate MAPE\n",
    "        #if end_date_mape:\n",
    "        #    mask = results['ds'] < end_date_mape\n",
    "        #    true_subset = results[\"true\"].loc[mask]\n",
    "        #    pred_subset = results[\"predicted\"].loc[mask]\n",
    "        #    metrics[\"mape\"] = mean_absolute_error(true_subset, pred_subset) / np.abs(true_subset).mean()\n",
    "        #else:\n",
    "        metrics[\"mape\"] = np.mean(np.abs((true_rescaled -pred_rescaled) / true_rescaled)) * 100\n",
    "\n",
    "        # Calculate information criteria if model is provided\n",
    "        if model:\n",
    "            residuals = results[\"true\"] - results[\"predicted\"]\n",
    "            n = len(residuals)\n",
    "            ssr = np.sum(residuals**2)\n",
    "            sigma2 = ssr / n  # MLE Estimator Variance\n",
    "\n",
    "            # Log-likelihood (normal distribution)\n",
    "            llf = -n/2 * np.log(2*np.pi) - n/2 * np.log(sigma2) - ssr/(2*sigma2)\n",
    "\n",
    "            # Count parameters\n",
    "            k = (len(model.params['k']) +        # growth parameters\n",
    "                 len(model.params['m']) +        # offset parameters\n",
    "                 len(model.params['sigma_obs']) + # observation noise\n",
    "                 (len(model.seasonalities) * 2) + # fourier terms for each seasonality\n",
    "                 len(model.extra_regressors))\n",
    "\n",
    "            metrics[\"aic\"] = -2 * llf + 2 * k\n",
    "            metrics[\"bic\"] = -2 * llf + k * np.log(n)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def visualize_metrics(self, metrics, figsize=(8, 3), title=\"Model Performance Metrics\"):\n",
    "        \"\"\"\n",
    "        Visualize evaluation metrics in a table.\n",
    "\n",
    "        Args:\n",
    "            metrics: Dictionary of metrics\n",
    "            figsize: Figure size\n",
    "            title: Plot title\n",
    "\n",
    "        Returns:\n",
    "            matplotlib Figure object\n",
    "        \"\"\"\n",
    "        if not metrics:\n",
    "            return None, None\n",
    "\n",
    "        # Create visualization\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.axis('off')\n",
    "        ax.axis('tight')\n",
    "\n",
    "        # Prepare table data\n",
    "        metric_names = []\n",
    "        metric_values = []\n",
    "        metric_formats = {\n",
    "            'rmse': '{:,.2f}',\n",
    "            'mae': '{:,.2f}',\n",
    "            'mape': '{:.2f}%',\n",
    "            'aic': '{:,.1f}',\n",
    "            'bic': '{:,.1f}'\n",
    "        }\n",
    "\n",
    "        for metric, value in metrics.items():\n",
    "            metric_names.append(metric.upper())\n",
    "            if metric in metric_formats:\n",
    "                metric_values.append(metric_formats[metric].format(value))\n",
    "            else:\n",
    "                metric_values.append(f\"{value:.4f}\")\n",
    "\n",
    "        # Create table\n",
    "        table = ax.table(\n",
    "            cellText=list(zip(metric_names, metric_values)),\n",
    "            colLabels=['Métrica de Desempeño', 'Valor'],\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "\n",
    "        # Style table\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1, 1.5)\n",
    "\n",
    "        # Header styling\n",
    "        for j in range(2):\n",
    "            table[0, j].set_facecolor('#40466e')\n",
    "            table[0, j].set_text_props(color='white', weight='bold')\n",
    "\n",
    "        # Alternate row colors\n",
    "        for i in range(len(metrics)):\n",
    "            color = '#f7f7f7' if i % 2 == 0 else 'white'\n",
    "            for j in range(2):\n",
    "                table[i+1, j].set_facecolor(color)\n",
    "\n",
    "        # Add title\n",
    "        ax.set_title(title, fontsize=14, pad=20, weight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def plot_forecast_comparison(self, results, test_data, horizon=3, title=None):\n",
    "        \"\"\"\n",
    "        Plot forecast vs actual values.\n",
    "\n",
    "        Args:\n",
    "            results: Forecast results DataFrame\n",
    "            test_data: Test data DataFrame\n",
    "            horizon: Forecast horizon\n",
    "            title: Plot title\n",
    "\n",
    "        Returns:\n",
    "            plotly Figure object\n",
    "        \"\"\"\n",
    "        if title is None:\n",
    "            title = f\"Multi-Step Forecast ({horizon}-Month Horizon) Thermal Generation\"\n",
    "\n",
    "        # Color mapping for different horizons\n",
    "        color_map = {\n",
    "            f\"Month {h}\": {\n",
    "                \"line\": color,\n",
    "                \"fill\": f\"rgba({rgb},{0.2})\"\n",
    "            }\n",
    "            for h, (color, rgb) in enumerate(zip(\n",
    "                [\"green\", \"orange\", \"red\"],\n",
    "                [\"0,255,0\", \"255,165,0\", \"255,0,0\"]\n",
    "            ), 1)\n",
    "        }\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add true values line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=test_data[\"ds\"],\n",
    "            y=self.rescale_values(test_data[\"y\"]),\n",
    "            mode=\"lines\",\n",
    "            name=\"Valores Medidos - Generación Eléctrica tipo Térmica\",\n",
    "            line=dict(color=\"blue\", width=2)\n",
    "        ))\n",
    "\n",
    "        # Add predictions for each horizon\n",
    "        for horizon_label in [f\"Month {h}\" for h in range(1, horizon + 1)]:\n",
    "            horizon_data = results[results[\"horizon\"] == horizon_label]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=horizon_data[\"ds\"],\n",
    "                y=self.rescale_values(horizon_data[\"predicted\"]),\n",
    "                mode=\"markers\",\n",
    "                name=horizon_label,\n",
    "                marker=dict(color=color_map[horizon_label]['line'], size=8),\n",
    "                hovertemplate=(\n",
    "                    \"<b>Date</b>: %{x}<br>\"\n",
    "                    \"<b>Predicted</b>: %{y:.2f}<br>\"\n",
    "                    f\"<b>Horizon</b>: {horizon_label}\"\n",
    "                )\n",
    "            ))\n",
    "\n",
    "        # Add vertical lines for forecast origins\n",
    "        for i in range(0, len(test_data), horizon):\n",
    "            fig.add_vline(\n",
    "                x=test_data[\"ds\"].iloc[i],\n",
    "                line_width=1,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"gray\"\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Energy [GWh]\",\n",
    "            hovermode=\"x unified\",\n",
    "            legend_title=\"Legend\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def rescale_values(self, series):\n",
    "        \"\"\"Rescale standardized values back to original scale.\"\"\"\n",
    "        return series * self.y_std + self.y_mean\n",
    "\n",
    "    def make_prediction(self, start_date = None):\n",
    "        if start_date is None:\n",
    "          prediction_date  = dt.now()\n",
    "          prediction_date = prediction_date.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "          prediction_date = start_date\n",
    "          prediction_date = pd.to_datetime(prediction_date)\n",
    "          prediction_date = prediction_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        #self.model = self.load_serialized_model('serialized_prophet')\n",
    "        raw_data = self.fetch_data(list_date, metric_Id_list, metric_Id_list_tar, entity_cov, entity_tar)\n",
    "        _, monthly_data = self.process_data()\n",
    "        monthly = self.load_and_concatenate('monthly_data.parquet', monthly_data, 'Fecha', True)\n",
    "        modeling_data, _ , _ , _ = self.preprocess_for_modeling(monthly)\n",
    "        delay = 1\n",
    "        number_of_lags = 3\n",
    "        final_lag = delay + number_of_lags\n",
    "        mod_modeling_data = self.mod_df(modeling_data, delay,number_of_lags, final_lag)\n",
    "        # Generate out-of-sample forecast\n",
    "        forecast_data, _ = self.generate_out_of_sample_forecast(self.model, mod_modeling_data, prediction_date)\n",
    "        return forecast_data, monthly, modeling_data\n",
    "\n",
    "    def forecast_data_rescaled(self,forecast_data, columns, y_mean, y_std):\n",
    "        forecast_data_rescale = forecast_data.copy()\n",
    "        for col in columns:\n",
    "          forecast_data_rescale[col] = forecast_data[col] * y_std + y_mean\n",
    "        return forecast_data_rescale\n",
    "\n",
    "    def save_forecast_data(self, forecast_data, columns, output_file):\n",
    "        \"\"\"\n",
    "        Save forecast data to a file.\n",
    "\n",
    "        Args:\n",
    "            forecast_data: Forecast DataFrame\n",
    "            file_path: Path to save file\n",
    "        \"\"\"\n",
    "        #columns_to_save = ['ds', 'trend', 'yhat_lower', 'yhat_upper']\n",
    "        columns_to_save = columns\n",
    "        selected_data = forecast_data[columns_to_save]\n",
    "        #data_dict = selected_data.to_dict(orient='records')\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        #forecast_data.to_csv(file_path, index=False)\n",
    "        try:\n",
    "            selected_data.to_csv(\n",
    "                output_path,\n",
    "                index= True,\n",
    "                date_format=\"%Y-%m-%d\"\n",
    "            )\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"Failed to write CSV file: {e}\")\n",
    "\n",
    "    def generate_out_of_sample_forecast(self, model, test_df, date_backtesting, future_periods=3):\n",
    "        \"\"\"\n",
    "        Generate out-of-sample forecast with exogenous variables.\n",
    "\n",
    "        Args:\n",
    "            test_df: Test DataFrame\n",
    "            future_periods: Number of periods to forecast\n",
    "            current_date: Current for backtesting\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (forecast DataFrame, exogenous data DataFrame)\n",
    "        \"\"\"\n",
    "        date_backtesting = pd.to_datetime(date_backtesting)\n",
    "        # Get last date from historical data\n",
    "        if test_df['ds'].iloc[-1] < date_backtesting:\n",
    "            #last_rows_set = test_df.iloc[-2:]\n",
    "            last_date = test_df['ds'].iloc[-1]\n",
    "\n",
    "\n",
    "            # Create block matrix for exogenous variables\n",
    "            row_input_list = np.array(test_df.iloc[-1][2:]).reshape(5,3)\n",
    "            result_matrix = self._create_block_matrix(row_input_list, num_blocks=5)\n",
    "\n",
    "            # Create future exogenous DataFrame\n",
    "            dates = pd.date_range(start=last_date, periods=3, freq='M')\n",
    "            df_future_exog = pd.DataFrame(\n",
    "                result_matrix,\n",
    "                columns=test_df.columns[2:],  # Assuming exog columns start from index 2\n",
    "                index=dates\n",
    "            )\n",
    "\n",
    "            # Interpolate missing values\n",
    "            df_future_exog_interpolate = df_future_exog.interpolate(method='linear', limit_direction='forward')\n",
    "            df_future_exog_interpolate.reset_index(inplace=True)\n",
    "            df_future_exog_interpolate.rename(columns={'index': 'ds'}, inplace=True)\n",
    "            forecast = model.predict(df_future_exog_interpolate)\n",
    "        else:\n",
    "            target_date = date_backtesting\n",
    "            idx_pos = test_df[test_df['ds'] == target_date].index[0]\n",
    "            test_df = test_df.drop(columns= 'y')\n",
    "            df_future_exog_interpolate = test_df.iloc[idx_pos-3:idx_pos]\n",
    "\n",
    "            #previous_3_rows = test_df.iloc[idx_pos-3:idx_pos]\n",
    "            #df_future_exog_interpolate = test_df[test_df['ds'] > target_date].tail(3)\n",
    "            forecast = model.predict(df_future_exog_interpolate)\n",
    "\n",
    "\n",
    "        return forecast, df_future_exog_interpolate\n",
    "\n",
    "    def _create_block_matrix(self, row_list, num_blocks=5):\n",
    "        \"\"\"\n",
    "        Creates a 3x(3*num_blocks) matrix by concatenating Toeplitz blocks.\n",
    "        \"\"\"\n",
    "        if num_blocks > len(row_list):\n",
    "            print(f\"Warning: num_blocks ({num_blocks}) exceeds the number of provided row lists ({len(row_list)}). Using {len(row_list)} blocks.\")\n",
    "            num_blocks = len(row_list)\n",
    "\n",
    "        # Create first block\n",
    "        first_row = row_list[0]\n",
    "        toeplitz_block = self._create_toeplitz(first_row)\n",
    "        block_size = len(first_row)\n",
    "        mask = np.tri(block_size, k=-1, dtype=bool)  # Lower triangular (excluding diagonal)\n",
    "        upper_block = np.where(~mask, toeplitz_block, np.nan)\n",
    "\n",
    "        # Concatenate blocks horizontally\n",
    "        full_matrix = upper_block.copy()\n",
    "        for i in range(1,num_blocks):\n",
    "            current_row = row_list[i]\n",
    "            if len(current_row) != block_size:\n",
    "                raise ValueError(f\"Row list at index {i} has inconsistent length. Expected {block_size}, got {len(current_row)}.\")\n",
    "            new_block = self._create_toeplitz(current_row)\n",
    "            new_block = np.where(~mask, new_block, np.nan)\n",
    "            full_matrix = np.hstack((full_matrix, new_block))\n",
    "\n",
    "        return full_matrix\n",
    "\n",
    "    def _create_toeplitz(self, row_list):\n",
    "        \"\"\"\n",
    "        Constructs a Toeplitz matrix from the given list representing the first row.\n",
    "        \"\"\"\n",
    "        num_cols = len(row_list)\n",
    "        num_rows = num_cols  # For a square Toeplitz matrix\n",
    "        toeplitz_matrix = np.zeros((num_rows, num_cols), dtype=row_list[0].__class__)\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                if j-i >= 0:\n",
    "                    toeplitz_matrix[i, j] = row_list[j-i]\n",
    "                elif i-j < num_cols:\n",
    "                    toeplitz_matrix[i, j] = row_list[i-j]\n",
    "                else:\n",
    "                    toeplitz_matrix[i, j] = np.nan\n",
    "        return toeplitz_matrix\n",
    "\n",
    "    def _predict_out_of_sample(self, model, last_train_date, exogenous_data, future_periods, freq):\n",
    "        \"\"\"\n",
    "        Generate out-of-sample forecasts using a trained Prophet model.\n",
    "        \"\"\"\n",
    "        # Create future dataframe\n",
    "        future = model.make_future_dataframe(\n",
    "            periods=future_periods,\n",
    "            freq=freq,\n",
    "            include_history=False\n",
    "        )\n",
    "\n",
    "        # Merge exogenous data\n",
    "        future = future.merge(exogenous_data, on='ds', how='left')\n",
    "\n",
    "        # Generate forecast\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def plot_time_series_with_forecast(self, test_data, forecast_data,\n",
    "                                     actual_series_name=\"Generación Térmica SIMEM\",\n",
    "                                     forecast_series_name=\"Predicción Fuera de la Muestra\",\n",
    "                                     confidence_band_name=\"Intervalo de Confianza\",\n",
    "                                     title=\"Serie de Tiempo para Generación Térmica Eléctrica + Predicción Fuera de la Muestra\",\n",
    "                                     xaxis_title=\"Fecha\",\n",
    "                                     yaxis_title=\"Energía [GWh]\",\n",
    "                                     confidence_band_color=\"rgba(0,100,80,0.2)\",\n",
    "                                     font_family=\"Times New Roman, serif\",\n",
    "                                     font_size=25,\n",
    "                                     template=\"plotly_white\",\n",
    "                                     forecast_line_style=\"dot\"\n",
    "                                    ):\n",
    "        \"\"\"\n",
    "        Plot time series data with forecast and confidence interval.\n",
    "        \"\"\"\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add actual values trace\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=test_data['ds'].iloc[:-1],\n",
    "            y=self.rescale_values(test_data['y'].iloc[:-1]),\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='black', width=3),\n",
    "            name=actual_series_name\n",
    "        ))\n",
    "\n",
    "        # Add forecast trace\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=forecast_data['ds'],\n",
    "            y=self.rescale_values(forecast_data['yhat']),\n",
    "            mode='lines+markers',\n",
    "            name=forecast_series_name,\n",
    "            line=dict(dash=forecast_line_style)\n",
    "        ))\n",
    "\n",
    "        # Add confidence interval band\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([forecast_data['ds'], forecast_data['ds'][::-1]]),\n",
    "            y=np.concatenate([\n",
    "                self.rescale_values(forecast_data['yhat_upper']),\n",
    "                self.rescale_values(forecast_data['yhat_lower'][::-1])\n",
    "            ]),\n",
    "            fill='toself',\n",
    "            fillcolor=confidence_band_color,\n",
    "            line=dict(color='rgba(255, 255, 255, 0)'),\n",
    "            name=confidence_band_name\n",
    "        ))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            xaxis_title=xaxis_title,\n",
    "            yaxis_title=yaxis_title,\n",
    "            font=dict(family=font_family, size=font_size),\n",
    "            template=template\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    def save_data(self, df, file_path):\n",
    "        \"\"\"\n",
    "        Save a DataFrame to NPZ file.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            file_path: Path to save file\n",
    "        \"\"\"\n",
    "        np.savez(\n",
    "            file_path,\n",
    "            index=df.index.values,\n",
    "            **{col: df[col].values for col in df.columns}\n",
    "        )\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load a DataFrame from NPZ file.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to NPZ file\n",
    "\n",
    "        Returns:\n",
    "            Loaded DataFrame\n",
    "        \"\"\"\n",
    "        with np.load(file_path, allow_pickle=True) as data:\n",
    "            df = pd.DataFrame(\n",
    "                {col: data[col] for col in data.files if col != 'index'},\n",
    "                index=data['index'] if 'index' in data.files else None\n",
    "            )\n",
    "        return df\n",
    "\n",
    "    def generate_descriptive_stats(self, data, periods):\n",
    "        \"\"\"\n",
    "        Generate descriptive statistics for specified time periods\n",
    "\n",
    "        Args:\n",
    "            data: pandas DataFrame with datetime index\n",
    "            periods: list of tuples with (period_name, years)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of DataFrames with descriptive statistics\n",
    "        \"\"\"\n",
    "        #data = data.set_index('Fecha',inplace=True)\n",
    "        results = {}\n",
    "        #current_date = data.index.max()  # Most recent date in data\n",
    "        current_date = data.index.max()\n",
    "\n",
    "        for period_name, years in periods:\n",
    "            # Calculate cutoff date\n",
    "            cutoff = current_date - pd.DateOffset(years=years)\n",
    "\n",
    "            # Filter data for the period\n",
    "            period_data = data.loc[cutoff:current_date]\n",
    "\n",
    "            # Generate descriptive stats\n",
    "            stats = period_data.describe(percentiles=[.25, .5, .75])\n",
    "\n",
    "            # Add additional statistics if needed\n",
    "            stats.loc['skew'] = period_data.skew()\n",
    "            stats.loc['kurtosis'] = period_data.kurtosis()\n",
    "            stats.loc['count_null'] = period_data.isnull().sum()\n",
    "\n",
    "            results[f\"{period_name} ({years} year)\"] = stats\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_to_parquet_by_date_range( self, df, output_path, start_date, end_date):\n",
    "        self.df = df.copy()\n",
    "        df_filtered = self.df.copy()\n",
    "        if start_date:\n",
    "            start_dt = start_date\n",
    "            df_filtered = df_filtered[df_filtered['Fecha'] >= start_dt]\n",
    "\n",
    "        if end_date:\n",
    "            end_dt = pd.to_datetime(end_date)\n",
    "            df_filtered = df_filtered[df_filtered['Fecha'] <= end_dt]\n",
    "\n",
    "        # Save to Parquet\n",
    "        df_filtered.to_parquet(\n",
    "            output_path,\n",
    "            #partition_cols=partition_cols,\n",
    "        )\n",
    "        print(f\"Successfully saved {len(df_filtered)} records to {output_path}\")\n",
    "\n",
    "\n",
    "    def load_and_concatenate( self, full_path, next_df , date_column, ensure_continuity):\n",
    "\n",
    "        self.date_column = date_column\n",
    "\n",
    "        # Construct full path if base_path is set\n",
    "        #full_path = file_path if self.base_path is None else f\"{self.base_path}/{file_path}\"\n",
    "\n",
    "        # Load the Parquet file\n",
    "        current_df = pd.read_parquet(full_path)\n",
    "\n",
    "        # Ensure date column exists and is datetime\n",
    "        if date_column not in current_df.columns:\n",
    "            raise ValueError(f\"Date column '{date_column}' not found in Parquet file\")\n",
    "\n",
    "        current_df[date_column] = pd.to_datetime(current_df[date_column])\n",
    "\n",
    "        # If no next_df provided, just return the loaded DataFrame\n",
    "        if next_df is None:\n",
    "            self.df = current_df\n",
    "            return current_df\n",
    "\n",
    "        # Validate next_df structure matches current_df\n",
    "        if set(current_df.columns) != set(next_df.columns):\n",
    "            raise ValueError(\"DataFrames must have identical column structures\")\n",
    "\n",
    "        # Ensure date column in next_df is datetime\n",
    "        #next_df[date_column] = pd.to_datetime(next_df[date_column])\n",
    "\n",
    "        # Check for datetime continuity if required\n",
    "        if ensure_continuity:\n",
    "            current_max_date = current_df[date_column].max()\n",
    "            next_min_date = next_df[date_column].min()\n",
    "\n",
    "            if next_min_date <= current_max_date:\n",
    "                raise ValueError(\n",
    "                    f\"Date continuity violation: Next DataFrame starts at {next_min_date} \"\n",
    "                    f\"which is before/equal to current DataFrame's max date {current_max_date}\"\n",
    "                )\n",
    "\n",
    "        # Concatenate while preserving original column order\n",
    "        concatenated = pd.concat(\n",
    "            [current_df, next_df],\n",
    "            axis=0,\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Restore original column order\n",
    "        concatenated = concatenated[current_df.columns]\n",
    "\n",
    "        # Update instance DataFrame\n",
    "        self.df = concatenated\n",
    "\n",
    "        return concatenated\n",
    "\n",
    "    @staticmethod\n",
    "    def load_serialized_figure(filepath):\n",
    "        \"\"\"Load a pickled Plotly figure\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            fig = pickle.load(f)\n",
    "        return fig\n",
    "    @staticmethod\n",
    "    def load_serialized_model(filepath):\n",
    "        \"\"\"Load a pickled Prophet model\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        return model\n",
    "    @staticmethod\n",
    "    def load_json_metrics (filepath):\n",
    "      df = pd.read_json(filepath, orient='columns')\n",
    "      return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7edd709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Initializing object\n",
      "The object has been initialized with the dataset: \"Generación Real y Programada en las Plantas de Generación\"\n",
      "****************************************************************************************************\n",
      "Inicio consulta sincronica\n",
      "Creacion url: 0.001445770263671875\n",
      "Extraccion de registros: 13.153762340545654\n",
      "End of data extracting process\n",
      "****************************************************************************************************\n",
      "\n",
      "1 AÑO (1 YEAR) Estadística Descriptiva de la Variables del Modelo\n",
      "                TERMICA  Volume in Energy  ONI Anomaly   AporEnergia  \\\n",
      "count         13.000000         13.000000    13.000000     13.000000   \n",
      "mean        1483.809031     297323.423077    -0.131538   6783.620246   \n",
      "std          672.307479      43688.448805     0.212165   2756.561922   \n",
      "min          717.301294     204915.420900    -0.390000   3701.567300   \n",
      "25%         1092.758092     273337.141000    -0.270000   4548.142700   \n",
      "50%         1145.757600     285567.999100    -0.160000   6345.871500   \n",
      "75%         1904.165195     322718.391100    -0.140000   8732.233800   \n",
      "max         2827.774631     359634.600400     0.450000  12541.164400   \n",
      "skew           0.750992         -0.333155     1.832304      0.770692   \n",
      "kurtosis      -0.489725          0.173773     4.381779     -0.326153   \n",
      "count_null     0.000000          0.000000     0.000000      0.000000   \n",
      "\n",
      "            PorcVoluUtilDiario  \n",
      "count                13.000000  \n",
      "mean                  0.565526  \n",
      "std                   0.080831  \n",
      "min                   0.380780  \n",
      "25%                   0.515903  \n",
      "50%                   0.568199  \n",
      "75%                   0.626385  \n",
      "max                   0.675202  \n",
      "skew                 -0.644012  \n",
      "kurtosis              0.950063  \n",
      "count_null            0.000000  \n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "2 AÑOS (2 YEAR) Estadística Descriptiva de la Variables del Modelo\n",
      "                TERMICA  Volume in Energy  ONI Anomaly   AporEnergia  \\\n",
      "count         25.000000         25.000000    25.000000     25.000000   \n",
      "mean        1779.007456     321011.698156     0.682400   5753.200784   \n",
      "std          705.353578      75521.580448     0.961146   2583.180585   \n",
      "min          717.301294     159696.170400    -0.390000   2178.585400   \n",
      "25%         1145.757600     273337.141000    -0.160000   3988.969600   \n",
      "50%         1609.976284     322718.391100     0.450000   4793.252800   \n",
      "75%         2344.339385     380403.992800     1.600000   7256.208000   \n",
      "max         3190.094600     461127.680900     2.150000  12541.164400   \n",
      "skew           0.290027         -0.219977     0.435281      0.824440   \n",
      "kurtosis      -0.917561         -0.338427    -1.564787      0.530630   \n",
      "count_null     0.000000          0.000000     0.000000      0.000000   \n",
      "\n",
      "            PorcVoluUtilDiario  \n",
      "count                25.000000  \n",
      "mean                  0.601378  \n",
      "std                   0.131807  \n",
      "min                   0.306643  \n",
      "25%                   0.515903  \n",
      "50%                   0.604467  \n",
      "75%                   0.699054  \n",
      "max                   0.818262  \n",
      "skew                 -0.456248  \n",
      "kurtosis             -0.161887  \n",
      "count_null            0.000000  \n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "5 AÑOS (5 YEAR) Estadística Descriptiva de la Variables del Modelo\n",
      "                TERMICA  Volume in Energy  ONI Anomaly   AporEnergia  \\\n",
      "count         61.000000         61.000000    61.000000     61.000000   \n",
      "mean        1353.091914     350424.862348    -0.133607   5941.440828   \n",
      "std          617.345350      78168.189902     0.964270   2266.801176   \n",
      "min          670.903217     159696.170400    -1.310000   2178.585400   \n",
      "25%          885.034730     295886.558700    -0.790000   4147.033000   \n",
      "50%         1145.757600     353350.460300    -0.390000   5964.983000   \n",
      "75%         1590.728620     422226.582000     0.050000   7256.208000   \n",
      "max         3190.094600     468144.821900     2.150000  12541.164400   \n",
      "skew           1.230859         -0.429203     1.206190      0.482613   \n",
      "kurtosis       0.772665         -0.402892     0.463424      0.052719   \n",
      "count_null     0.000000          0.000000     0.000000      0.000000   \n",
      "\n",
      "            PorcVoluUtilDiario  \n",
      "count                61.000000  \n",
      "mean                  0.664313  \n",
      "std                   0.142480  \n",
      "min                   0.306643  \n",
      "25%                   0.573229  \n",
      "50%                   0.671229  \n",
      "75%                   0.774173  \n",
      "max                   0.866936  \n",
      "skew                 -0.514870  \n",
      "kurtosis             -0.195961  \n",
      "count_null            0.000000  \n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = ThermalGenerationPipeline()\n",
    "\n",
    "# Fetch data - from the API\n",
    "list_date = [dt.fromisoformat('2025-02-01'), dt.fromisoformat('2025-05-31')]\n",
    "metric_Id_list = ['VoluUtilDiarEner','AporEner','PorcVoluUtilDiar']\n",
    "metric_Id_list_tar = ['Gene']\n",
    "entity_cov = 'Sistema'\n",
    "entity_tar = 'Recurso'\n",
    "\n",
    "\n",
    "# Make the forecast - Preprocess-Concatenate\n",
    "#date_backtesting = '2015-02-28'\n",
    "#out_of_sample_forecast, monthly_data, test_df = pipeline.make_prediction(start_date = date_backtesting)\n",
    "out_of_sample_forecast, monthly_data, test_df = pipeline.make_prediction()\n",
    "\n",
    "# Rescale the forecast according to the  standarization method\n",
    "forecast_data_rescaled = pipeline.forecast_data_rescaled(out_of_sample_forecast,['yhat', 'yhat_lower', 'yhat_upper'],\n",
    "                                                         pipeline.y_mean, pipeline.y_std)\n",
    "\n",
    "# Save the forecast and the measured data into a csv file\n",
    "pipeline.save_forecast_data(forecast_data_rescaled,['ds', 'yhat', 'yhat_lower', 'yhat_upper'], 'forecast_data.csv')\n",
    "pipeline.save_forecast_data(monthly_data,monthly_data.columns,'measured_data.csv')\n",
    "\n",
    "\n",
    "# Load the test set metrics\n",
    "metrics = pipeline.load_json_metrics('metrics_test.json')\n",
    "\n",
    "# Plot the forecast\n",
    "forecast_fig = pipeline.plot_time_series_with_forecast(\n",
    "    test_df,\n",
    "    out_of_sample_forecast,\n",
    ")\n",
    "\n",
    "\n",
    "# Periods to analyze 1-year, 2-year, 5-year.\n",
    "periods_to_analyze = [\n",
    "    ('1 año', 1),\n",
    "    ('2 años', 2),\n",
    "    ('5 años', 5)\n",
    "]\n",
    "\n",
    "# Generate the reports\n",
    "monthly_data.set_index('Fecha',inplace=True)\n",
    "statistical_reports = pipeline.generate_descriptive_stats(monthly_data, periods_to_analyze)\n",
    "\n",
    "# Access the reports\n",
    "for period_name, report in statistical_reports.items():\n",
    "    print(f\"\\n{period_name.upper()} Estadística Descriptiva de la Variables del Modelo\")\n",
    "    print(report)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b96a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "black",
          "width": 3
         },
         "mode": "lines+markers",
         "name": "Generación Térmica SIMEM",
         "type": "scatter",
         "x": [
          "2013-04-30T00:00:00.000000000",
          "2013-05-31T00:00:00.000000000",
          "2013-06-30T00:00:00.000000000",
          "2013-07-31T00:00:00.000000000",
          "2013-08-31T00:00:00.000000000",
          "2013-09-30T00:00:00.000000000",
          "2013-10-31T00:00:00.000000000",
          "2013-11-30T00:00:00.000000000",
          "2013-12-31T00:00:00.000000000",
          "2014-01-31T00:00:00.000000000",
          "2014-02-28T00:00:00.000000000",
          "2014-03-31T00:00:00.000000000",
          "2014-04-30T00:00:00.000000000",
          "2014-05-31T00:00:00.000000000",
          "2014-06-30T00:00:00.000000000",
          "2014-07-31T00:00:00.000000000",
          "2014-08-31T00:00:00.000000000",
          "2014-09-30T00:00:00.000000000",
          "2014-10-31T00:00:00.000000000",
          "2014-11-30T00:00:00.000000000",
          "2014-12-31T00:00:00.000000000",
          "2015-01-31T00:00:00.000000000",
          "2015-02-28T00:00:00.000000000",
          "2015-03-31T00:00:00.000000000",
          "2015-04-30T00:00:00.000000000",
          "2015-05-31T00:00:00.000000000",
          "2015-06-30T00:00:00.000000000",
          "2015-07-31T00:00:00.000000000",
          "2015-08-31T00:00:00.000000000",
          "2015-09-30T00:00:00.000000000",
          "2015-10-31T00:00:00.000000000",
          "2015-11-30T00:00:00.000000000",
          "2015-12-31T00:00:00.000000000",
          "2016-01-31T00:00:00.000000000",
          "2016-02-29T00:00:00.000000000",
          "2016-03-31T00:00:00.000000000",
          "2016-04-30T00:00:00.000000000",
          "2016-05-31T00:00:00.000000000",
          "2016-06-30T00:00:00.000000000",
          "2016-07-31T00:00:00.000000000",
          "2016-08-31T00:00:00.000000000",
          "2016-09-30T00:00:00.000000000",
          "2016-10-31T00:00:00.000000000",
          "2016-11-30T00:00:00.000000000",
          "2016-12-31T00:00:00.000000000",
          "2017-01-31T00:00:00.000000000",
          "2017-02-28T00:00:00.000000000",
          "2017-03-31T00:00:00.000000000",
          "2017-04-30T00:00:00.000000000",
          "2017-05-31T00:00:00.000000000",
          "2017-06-30T00:00:00.000000000",
          "2017-07-31T00:00:00.000000000",
          "2017-08-31T00:00:00.000000000",
          "2017-09-30T00:00:00.000000000",
          "2017-10-31T00:00:00.000000000",
          "2017-11-30T00:00:00.000000000",
          "2017-12-31T00:00:00.000000000",
          "2018-01-31T00:00:00.000000000",
          "2018-02-28T00:00:00.000000000",
          "2018-03-31T00:00:00.000000000",
          "2018-04-30T00:00:00.000000000",
          "2018-05-31T00:00:00.000000000",
          "2018-06-30T00:00:00.000000000",
          "2018-07-31T00:00:00.000000000",
          "2018-08-31T00:00:00.000000000",
          "2018-09-30T00:00:00.000000000",
          "2018-10-31T00:00:00.000000000",
          "2018-11-30T00:00:00.000000000",
          "2018-12-31T00:00:00.000000000",
          "2019-01-31T00:00:00.000000000",
          "2019-02-28T00:00:00.000000000",
          "2019-03-31T00:00:00.000000000",
          "2019-04-30T00:00:00.000000000",
          "2019-05-31T00:00:00.000000000",
          "2019-06-30T00:00:00.000000000",
          "2019-07-31T00:00:00.000000000",
          "2019-08-31T00:00:00.000000000",
          "2019-09-30T00:00:00.000000000",
          "2019-10-31T00:00:00.000000000",
          "2019-11-30T00:00:00.000000000",
          "2019-12-31T00:00:00.000000000",
          "2020-01-31T00:00:00.000000000",
          "2020-02-29T00:00:00.000000000",
          "2020-03-31T00:00:00.000000000",
          "2020-04-30T00:00:00.000000000",
          "2020-05-31T00:00:00.000000000",
          "2020-06-30T00:00:00.000000000",
          "2020-07-31T00:00:00.000000000",
          "2020-08-31T00:00:00.000000000",
          "2020-09-30T00:00:00.000000000",
          "2020-10-31T00:00:00.000000000",
          "2020-11-30T00:00:00.000000000",
          "2020-12-31T00:00:00.000000000",
          "2021-01-31T00:00:00.000000000",
          "2021-02-28T00:00:00.000000000",
          "2021-03-31T00:00:00.000000000",
          "2021-04-30T00:00:00.000000000",
          "2021-05-31T00:00:00.000000000",
          "2021-06-30T00:00:00.000000000",
          "2021-07-31T00:00:00.000000000",
          "2021-08-31T00:00:00.000000000",
          "2021-09-30T00:00:00.000000000",
          "2021-10-31T00:00:00.000000000",
          "2021-11-30T00:00:00.000000000",
          "2021-12-31T00:00:00.000000000",
          "2022-01-31T00:00:00.000000000",
          "2022-02-28T00:00:00.000000000",
          "2022-03-31T00:00:00.000000000",
          "2022-04-30T00:00:00.000000000",
          "2022-05-31T00:00:00.000000000",
          "2022-06-30T00:00:00.000000000",
          "2022-07-31T00:00:00.000000000",
          "2022-08-31T00:00:00.000000000",
          "2022-09-30T00:00:00.000000000",
          "2022-10-31T00:00:00.000000000",
          "2022-11-30T00:00:00.000000000",
          "2022-12-31T00:00:00.000000000",
          "2023-01-31T00:00:00.000000000",
          "2023-02-28T00:00:00.000000000",
          "2023-03-31T00:00:00.000000000",
          "2023-04-30T00:00:00.000000000",
          "2023-05-31T00:00:00.000000000",
          "2023-06-30T00:00:00.000000000",
          "2023-07-31T00:00:00.000000000",
          "2023-08-31T00:00:00.000000000",
          "2023-09-30T00:00:00.000000000",
          "2023-10-31T00:00:00.000000000",
          "2023-11-30T00:00:00.000000000",
          "2023-12-31T00:00:00.000000000",
          "2024-01-31T00:00:00.000000000",
          "2024-02-29T00:00:00.000000000",
          "2024-03-31T00:00:00.000000000",
          "2024-04-30T00:00:00.000000000",
          "2024-05-31T00:00:00.000000000",
          "2024-06-30T00:00:00.000000000",
          "2024-07-31T00:00:00.000000000",
          "2024-08-31T00:00:00.000000000",
          "2024-09-30T00:00:00.000000000",
          "2024-10-31T00:00:00.000000000",
          "2024-11-30T00:00:00.000000000",
          "2024-12-31T00:00:00.000000000",
          "2025-01-31T00:00:00.000000000",
          "2025-02-28T00:00:00.000000000",
          "2025-03-31T00:00:00.000000000",
          "2025-04-30T00:00:00.000000000"
         ],
         "y": {
          "bdata": "R1yQc9WYmUCTNNuzzKeUQAN44HJXxpNAM/GbrWefmkABsu+KQw2WQJ+jFXqOQZJAIcN+Mq+1lkDyjR0AfzCYQGcXOkVk+JVAs5sI6QGfmECLeMuv3wWYQO5wI9I3O5VAEjbCiF5Om0CvQAjnrHSeQL5/MA325ZxAWAnpZ86cmECjzhSBn76XQK4AtfTMbZZAkW5fcBMymUBYnVL4JmiVQBpLH0wmyJdAq9L7MdtTmkC7L0bWf46TQJJOlH71w5hA9aOJeW37kkDz356rHTCaQOmKZNzciJdAz1KqKOC4l0D4BnZUx/6VQIy1jOunIJ5AcGoRcgLwo0BSKbdozYOiQCLRiIDKRKRAYqdP45JNpUCBDvamchWkQMUpQ1k7S6ZAbv7gEF1znkC5ZJKr/NGRQPKP50NZ1pJAV7njN3Ypj0AVWUJ5J6CRQEAXykNxvI5Acg4n4pkCk0C4vykDYWyNQBS4was8m4hA61I2SSbOhEAlILw+VeKMQKDFSatcS4tAWCDyW6+dhED+yVx8N0WCQEdxCk86+oBA9OZqxPlagEC8xyV/mGuEQIgSTSzMqYhAndazihIxi0BaWntGO6SJQJItB/SFZYdA8Ipp1h64jEBf9zcduzeVQE4XCuM2D5VAdPuK8Z82h0Ca7K/IC8eJQFpdLBGspYhAQ/dtDyHbh0Ccrn/3x2mLQD5gTRXIEY1ArTb4dmeUjEBPpPzbliOIQO/ydlpBnpNAro3SOXWDmUAvDT5q3/6XQHCpzEVz0pZACLpqOX7hj0CHhkfkAOuNQCYB7uUP9YRAYkLPi7XyhkDqnG50Z36MQDIVG1iQNZNAnuvILNNYl0BWQcXo0h6VQBK8zp/7jJtAziHvCERAnUAP9oh72W2hQKOb3F3VEqBADKK3BmrmlkBAXrxt46+eQGe4Lti/dJxAOYLkYlzCjUBzJnJxupeQQENSC/E9a5RAbUuNjWmTlUCwXaWUPE+SQLqWrQXoJZNAcJUY4K9nk0CSLljN8zmYQPbMxObzc5FAdTftmEefjUBuPZtag8mIQPhNuCMORIlAWMC47+GXjEAqW0B8asqLQCan2EyT7YpAwkqqenO2jEDqhWxlmpyKQFQeEzpwPJVAMjuqOSBElUAhcYFwvrmVQFwiY6IeHo5Aet5gEGy9ikAEgvJBspuLQBVNNxEYIopA2ZQDVnPujEAkUfT0FHiJQAt/Faw5iolAdMbw0mVhhUA8MCH/xY2FQNw4+1CK945A1hK8WTlTjUBv5Bh58veSQAXceLO4sopAidHV2t3khEBHCj5RsNKWQJCAwFiIxJNA25GIYFDrl0AYgq7hJN6YQKEsGfiq56JAf2hJ70CNpEDfz8eT+oqZQDYvh23sMqNAfnv9xQ88oEAMsKM49WSgQF+mvD+OSKZAbNDDopf4qEBocldGLGyRQFr1gECLD5FAQcEFdUi0kUDCbcP2ZCuZQN4i7Wquu6FAdwXHJoIhpkDuFwluHsidQGSYKYBgV6JAIUnEiA0jmEDsYQrKQeSRQIeeXgd2xYlApRuFqu2zh0A=",
          "dtype": "f8"
         }
        },
        {
         "line": {
          "dash": "dot"
         },
         "mode": "lines+markers",
         "name": "Predicción Fuera de la Muestra",
         "type": "scatter",
         "x": [
          "2025-05-31T00:00:00.000000000",
          "2025-06-30T00:00:00.000000000",
          "2025-07-31T00:00:00.000000000"
         ],
         "y": {
          "bdata": "l9tixhlLj0B7rhGb1Z2TQCSHuO+DeJZA",
          "dtype": "f8"
         }
        },
        {
         "fill": "toself",
         "fillcolor": "rgba(0,100,80,0.2)",
         "line": {
          "color": "rgba(255, 255, 255, 0)"
         },
         "name": "Intervalo de Confianza",
         "type": "scatter",
         "x": [
          "2025-05-31T00:00:00.000000000",
          "2025-06-30T00:00:00.000000000",
          "2025-07-31T00:00:00.000000000",
          "2025-07-31T00:00:00.000000000",
          "2025-06-30T00:00:00.000000000",
          "2025-05-31T00:00:00.000000000"
         ],
         "y": {
          "bdata": "7HBMk+kNlUDyBZz+8M6YQCpVRPva+5tAwIb2ITnZkECqvs3mn6KMQAZLRLNh5oRA",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "font": {
         "family": "Times New Roman, serif",
         "size": 25
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Serie de Tiempo para Generación Térmica Eléctrica + Predicción Fuera de la Muestra"
        },
        "xaxis": {
         "title": {
          "text": "Fecha"
         }
        },
        "yaxis": {
         "title": {
          "text": "Energía [GWh]"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forecast_fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
